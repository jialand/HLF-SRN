Preparing Data ====================================>
Image count for train :48300
Image count for valid :21000
Loading Teacher ====================================>
loading RCANx4
Preparing Student ===================================>
Preparing loss function:
0.500 * DS
0.500 * TS
10.000 * SA
Model settings
Teachers: [RCAN]
Student: RCAN

Data Settings
RGB range: 255
Scale: 4
Input Image Size: (48, 48, 3)
Output Image Size: (192, 192, 3)

Training Settings
Epochs: 200
Learning rate: 0.000100
Learning rate decay: 150-300-450-600

Distillation Settings
Distillation type: 
	teacher supervision
	feature distillation
		type: SA
		position: [1,2,3]



Start Training ======================================>
epoch 1
X:\miniconda3\envs\FSR\lib\site-packages\torch\optim\lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
X:\miniconda3\envs\FSR\lib\site-packages\torch\optim\lr_scheduler.py:454: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
[Epoch 1]	Learning rate: 1.00e-04
[0/1000]	[DS: 55.1604][TS: 55.1611][SA: 1.4538][Total: 111.7752]	3.1+0.1s
Traceback (most recent call last):
  File "X:\FSR\Knowledge-Distillation-for-Super-resolution-master\code\train.py", line 421, in <module>
    train(epoch)
  File "X:\FSR\Knowledge-Distillation-for-Super-resolution-master\code\train.py", line 217, in train
    total_loss.backward()
  File "X:\miniconda3\envs\FSR\lib\site-packages\torch\_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "X:\miniconda3\envs\FSR\lib\site-packages\torch\autograd\__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
^C